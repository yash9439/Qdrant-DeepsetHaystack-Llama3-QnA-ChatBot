{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Installing Required Librarires**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install qdrant-haystack\n",
    "# !pip install fastembed\n",
    "# !pip install groq\n",
    "# !pip install gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from fastembed import TextEmbedding\n",
    "from groq import Groq\n",
    "import gradio as gr\n",
    "\n",
    "from haystack.dataclasses.document import Document\n",
    "from haystack_integrations.document_stores.qdrant import QdrantDocumentStore\n",
    "from haystack_integrations.components.retrievers.qdrant import QdrantEmbeddingRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scraping Documentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sitemap_data(url):\n",
    "    \"\"\"\n",
    "    Retrieves the sitemap.xml data from the given URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): The base URL of the documentation website.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the sitemap.xml file.\n",
    "    \"\"\"\n",
    "\n",
    "    sitemap_url = f\"{url}/sitemap.xml\"\n",
    "    try:\n",
    "        response = requests.get(sitemap_url)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching sitemap: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_urls_from_sitemap(sitemap_data):\n",
    "    \"\"\"\n",
    "    Extracts URLs from the given sitemap.xml data.\n",
    "\n",
    "    Args:\n",
    "        sitemap_data (str): The content of the sitemap.xml file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of URLs extracted from the sitemap.\n",
    "    \"\"\"\n",
    "\n",
    "    soup = BeautifulSoup(sitemap_data, 'xml')\n",
    "    urls = []\n",
    "    for url_tag in soup.find_all('url'):\n",
    "        loc_tag = url_tag.find('loc')\n",
    "        if loc_tag:\n",
    "            urls.append(loc_tag.text)\n",
    "    return urls\n",
    "\n",
    "def fetch_and_store_documentation(base_url):\n",
    "    \"\"\"\n",
    "    Fetches documentation content from URLs and stores them in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the documentation website.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are URLs and values are filtered HTML content.\n",
    "    \"\"\"\n",
    "\n",
    "    sitemap_data = get_sitemap_data(base_url)\n",
    "    if sitemap_data:\n",
    "        urls = extract_urls_from_sitemap(sitemap_data)\n",
    "        docs = {}  # Initialize an empty dictionary\n",
    "\n",
    "        for url in urls:\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                # Filter out unwanted tags using BeautifulSoup (adjust as needed)\n",
    "                for tag in ['script', 'style', 'nav', 'aside', 'footer']:\n",
    "                    for element in soup.find_all(tag):\n",
    "                        element.decompose()\n",
    "\n",
    "                docs[url] = soup.get_text(separator=' ')  # Store filtered HTML content\n",
    "                print(f\"Fetched and stored content from: {url}\")\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching {url}: {e}\")\n",
    "\n",
    "        return docs\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# Sentence Tokenization\n",
    "def fetch_and_processing(base_url):\n",
    "    documentation_data = fetch_and_store_documentation(base_url) \n",
    "    for url, content in documentation_data.items():\n",
    "        sentences = sent_tokenize(content)\n",
    "        documentation_data[url] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched and stored content from: https://llama-cpp-python.readthedocs.io/en/stable/\n",
      "Fetched and stored content from: https://llama-cpp-python.readthedocs.io/en/latest/\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://llama-cpp-python.readthedocs.io/\"\n",
    "documentation_data = fetch_and_processing(base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Embedding Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yash/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 110960.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TextEmbedding model\n",
    "embedding_model = TextEmbedding(model_name=\"BAAI/bge-small-en-v1.5\", cache_dir=\"./embeddings\")\n",
    "\n",
    "def embed_documents(documents):\n",
    "    for url, sentences in documentation_data.items():\n",
    "        \n",
    "        embeddings = []\n",
    "        for sentence in sentences:\n",
    "            # Embed document using FastEmbed\n",
    "            embedding = np.array(list((embedding_model.embed([sentence]))))\n",
    "            \n",
    "            # Append the embedding to the list of embeddings\n",
    "            embeddings.append((sentence,embedding))\n",
    "        \n",
    "        documentation_data[url] = embeddings\n",
    "        \n",
    "    return documentation_data\n",
    "\n",
    "# Perform embedding generation\n",
    "documentation_data = embed_documents(documentation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Creating a Vector Database using Qdrant on Haystack Framework**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 45427.32it/s]          \n"
     ]
    }
   ],
   "source": [
    "ingestion_data = []\n",
    "\n",
    "document_store = QdrantDocumentStore(\n",
    "    \":memory:\",\n",
    "    index=\"Document\",\n",
    "    embedding_dim=384,\n",
    "    recreate_index=True,\n",
    "    hnsw_config={\"m\": 16, \"ef_construct\": 64}  # Optional\n",
    ")\n",
    "\n",
    "for url, sentences in documentation_data.items():\n",
    "    # print(sentences[0][0])\n",
    "    ingestion_data.append(Document(content=sentences[0][0], embedding=sentences[0][1][0], meta={\"url\": url}))\n",
    "    try:\n",
    "        document_store.write_documents(ingestion_data)\n",
    "    except:\n",
    "        # Duplicate document\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = QdrantEmbeddingRetriever(document_store=document_store)\n",
    "\n",
    "query = \"How to install Llama-cpp ?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RAG with Llama 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groqInference(query):\n",
    "    query_embedding = list((embedding_model.embed([query])))\n",
    "    retrieved_content = retriever.run(list(query_embedding[0]))\n",
    "    \n",
    "    client = Groq(\n",
    "        api_key=\"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\",\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"Below is given a Documentation and answer the question asked in the end:\n",
    "    {retrieved_content['documents'][0].content}\n",
    "    \\n\\n\\n\n",
    "    {query}\n",
    "    \"\"\"\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama3-70b-8192\",\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = groqInference(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The question being asked is \"How to install Llama-cpp?\" However, the provided documentation does not explicitly answer this question. It appears to be an introduction or a README file for the llama-cpp-python project, which is a Python binding for the llama.cpp library.'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Gradio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 3.50.2, however version 4.29.0 is available, please upgrade.\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "iface = gr.Interface(\n",
    "    fn=groqInference,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Query\", placeholder=\"Enter your question here\")\n",
    "    ],\n",
    "    outputs=[gr.Textbox(label=\"Generated Response\")],\n",
    "    title=\"RAG with Qdrant, FastEmbed and Gemini\",\n",
    "    description=\"Enter thequestion to get a generated response based on the retrieved text from the Documentation.\"\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
