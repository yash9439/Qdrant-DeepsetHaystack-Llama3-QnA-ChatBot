{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Installing Required Librarires**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install qdrant-haystack\n",
    "# !pip install fastembed\n",
    "# !pip install groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from fastembed import TextEmbedding\n",
    "from groq import Groq\n",
    "\n",
    "from haystack.dataclasses.document import Document\n",
    "from haystack_integrations.document_stores.qdrant import QdrantDocumentStore\n",
    "from haystack_integrations.components.retrievers.qdrant import QdrantEmbeddingRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scraping Documentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sitemap_data(url):\n",
    "    \"\"\"\n",
    "    Retrieves the sitemap.xml data from the given URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): The base URL of the documentation website.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the sitemap.xml file.\n",
    "    \"\"\"\n",
    "\n",
    "    sitemap_url = f\"{url}/sitemap.xml\"\n",
    "    try:\n",
    "        response = requests.get(sitemap_url)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching sitemap: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_urls_from_sitemap(sitemap_data):\n",
    "    \"\"\"\n",
    "    Extracts URLs from the given sitemap.xml data.\n",
    "\n",
    "    Args:\n",
    "        sitemap_data (str): The content of the sitemap.xml file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of URLs extracted from the sitemap.\n",
    "    \"\"\"\n",
    "\n",
    "    soup = BeautifulSoup(sitemap_data, 'xml')\n",
    "    urls = []\n",
    "    for url_tag in soup.find_all('url'):\n",
    "        loc_tag = url_tag.find('loc')\n",
    "        if loc_tag:\n",
    "            urls.append(loc_tag.text)\n",
    "    return urls\n",
    "\n",
    "def fetch_and_store_documentation(base_url):\n",
    "    \"\"\"\n",
    "    Fetches documentation content from URLs and stores them in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the documentation website.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are URLs and values are filtered HTML content.\n",
    "    \"\"\"\n",
    "\n",
    "    sitemap_data = get_sitemap_data(base_url)\n",
    "    if sitemap_data:\n",
    "        urls = extract_urls_from_sitemap(sitemap_data)\n",
    "        docs = {}  # Initialize an empty dictionary\n",
    "\n",
    "        for url in urls:\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                # Filter out unwanted tags using BeautifulSoup (adjust as needed)\n",
    "                for tag in ['script', 'style', 'nav', 'aside', 'footer']:\n",
    "                    for element in soup.find_all(tag):\n",
    "                        element.decompose()\n",
    "\n",
    "                docs[url] = soup.get_text(separator=' ')  # Store filtered HTML content\n",
    "                print(f\"Fetched and stored content from: {url}\")\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching {url}: {e}\")\n",
    "\n",
    "        return docs\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched and stored content from: https://llama-cpp-python.readthedocs.io/en/stable/\n",
      "Fetched and stored content from: https://llama-cpp-python.readthedocs.io/en/latest/\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://llama-cpp-python.readthedocs.io/\"\n",
    "documentation_data = fetch_and_store_documentation(base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sentence Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences in https://llama-cpp-python.readthedocs.io/en/stable/: 144\n",
      "Sentences in https://llama-cpp-python.readthedocs.io/en/latest/: 144\n"
     ]
    }
   ],
   "source": [
    "for url, content in documentation_data.items():\n",
    "    sentences = sent_tokenize(content)\n",
    "    documentation_data[url] = sentences\n",
    "    print(f\"Sentences in {url}: {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Embedding Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yash/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 110960.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TextEmbedding model\n",
    "embedding_model = TextEmbedding(model_name=\"BAAI/bge-small-en-v1.5\", cache_dir=\"./embeddings\")\n",
    "\n",
    "def embed_documents(documents):\n",
    "    for url, sentences in documentation_data.items():\n",
    "        \n",
    "        embeddings = []\n",
    "        for sentence in sentences:\n",
    "            # Embed document using FastEmbed\n",
    "            embedding = np.array(list((embedding_model.embed([sentence]))))\n",
    "            \n",
    "            # Append the embedding to the list of embeddings\n",
    "            embeddings.append((sentence,embedding))\n",
    "        \n",
    "        documentation_data[url] = embeddings\n",
    "        \n",
    "    return documentation_data\n",
    "\n",
    "# Perform embedding generation\n",
    "documentation_data = embed_documents(documentation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Creating a Vector Database using Qdrant on Haystack Framework**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 45427.32it/s]          \n"
     ]
    }
   ],
   "source": [
    "ingestion_data = []\n",
    "\n",
    "document_store = QdrantDocumentStore(\n",
    "    \":memory:\",\n",
    "    index=\"Document\",\n",
    "    embedding_dim=384,\n",
    "    recreate_index=True,\n",
    "    hnsw_config={\"m\": 16, \"ef_construct\": 64}  # Optional\n",
    ")\n",
    "\n",
    "for url, sentences in documentation_data.items():\n",
    "    # print(sentences[0][0])\n",
    "    ingestion_data.append(Document(content=sentences[0][0], embedding=sentences[0][1][0], meta={\"url\": url}))\n",
    "    try:\n",
    "        document_store.write_documents(ingestion_data)\n",
    "    except:\n",
    "        # Duplicate document\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = QdrantEmbeddingRetriever(document_store=document_store)\n",
    "\n",
    "query = \"How to install Llama-cpp ?\"\n",
    "\n",
    "query_embedding = list((embedding_model.embed([query])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_content = retriever.run(list(query_embedding[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RAG with Llama 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(\n",
    "    api_key=\"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\",\n",
    ")\n",
    "\n",
    "prompt = f\"\"\"Below is given a Documentation and answer the question asked in the end:\n",
    "{retrieved_content['documents'][0].content}\n",
    "\\n\\n\\n\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-70b-8192\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the documentation, the question \"How to install Llama-cpp?\" is not directly answered in the provided text. The text is an introduction to the Python bindings for llama.cpp, but it does not provide installation instructions.\n"
     ]
    }
   ],
   "source": [
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
